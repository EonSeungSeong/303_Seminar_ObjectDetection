{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08e286eb",
   "metadata": {},
   "source": [
    "### 1. Import Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69951ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q --upgrade selectivesearch torch_snippets\n",
    "from torch_snippets import *\n",
    "import selectivesearch\n",
    "from torchvision import transforms, models, datasets\n",
    "from torch_snippets import Report\n",
    "from torchvision.ops import nms\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#device='cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc4a8c3",
   "metadata": {},
   "source": [
    "### 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce22e82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#image_root = '/home/user303/1_eonseung/Object_Detection/Covid_data_256/train'\n",
    "image_root = '/home/user303/ObjectDetection/Book/data/images/images/'\n",
    "\n",
    "#image_root2 = '/home/user303/1_eonseung/Object_Detection/Covid_data_256/'\n",
    "\n",
    "#data = pd.read_csv('/home/user303/2_sunghyun/OD/500od_data.csv')\n",
    "data = pd.read_csv('/home/user303/ObjectDetection/Book/data/df.csv')\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03294c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data['ImageID'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713444ff",
   "metadata": {},
   "source": [
    "### 3. Openimage Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb563a1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class OpenImages(Dataset):\n",
    "    def __init__(self, df, image_folder=image_root):\n",
    "        self.root = image_folder\n",
    "        self.df = df \n",
    "#         self.unique_images = df['id'].unique()\n",
    "        self.unique_images = df['ImageID'].unique()\n",
    "    def __len__(self): return len(self.unique_images)\n",
    "    def __getitem__(self, ix):\n",
    "        image_id = self.unique_images[ix] #파일 고유명\n",
    "        image_path = f'{self.root}/{image_id}.jpg' #이미지의 전체경로\n",
    "        #print(image_id)\n",
    "        #print(image_path)\n",
    "        image = cv2.imread(image_path, 1)[...,::-1] # convert BGR to RGB\n",
    "        #image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        #image=image.reshape(256,256,1)\n",
    "        #print(image.shape)\n",
    "        h, w,_ = image.shape # h=높이, w= 너비 _=채널(depth)\n",
    "        #print(h,w,_)\n",
    "        df = self.df.copy() #이미지 변형방지를 위한 복사\n",
    "        #df = df[df['id'] == image_id] \n",
    "        df = df[df['ImageID'] == image_id] \n",
    "        #boxes = df['frac_xmin,frac_ymin,frac_xmax,frac_ymax'.split(',')].values #csv 에서 정규화 되어있는 값 가져오기\n",
    "        boxes = df['XMin,YMin,XMax,YMax'.split(',')].values\n",
    "        boxes = (boxes * np.array([w,h,w,h])).astype(np.uint16).tolist() #정규화된 값 -> 정수->리스트 담기\n",
    "        #classes = df['human_label'].values.tolist() #분류된 box의 라벨명->리스트\n",
    "        classes = df['LabelName'].values.tolist()\n",
    "        #print(classes) \n",
    "        return image, boxes, classes, image_path\n",
    "ds = OpenImages(df=data)\n",
    "im, bbs, clss, _ = ds[450] #image(RGB), boxes(여러개), lablename, path\n",
    "#print(im)\n",
    "#print(type(bbs))\n",
    "#print(ds[2])\n",
    "show(im, bbs=bbs, texts=clss, sz=10)\n",
    "\n",
    "\"\"\"\n",
    "(XMin,Ymin)-> 이미지 좌측 하단 좌표의 정규화 값\n",
    "(XMax,Ymax)-> 이미지 우측 상단 좌표의 정규화 값\n",
    "네 값으로 bb 생성\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e637b35d",
   "metadata": {},
   "source": [
    "### 4. Region Proposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb7a38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "a = time.time()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d99381",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "help(selectivesearch.selective_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188b30c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_candidates(img): \n",
    "    img_lbl, regions = selectivesearch.selective_search(img,scale=200,sigma=0.8,min_size=100)\n",
    "    img_area = np.prod(img.shape[:2])#np.prod ->요소들의 곱 반환\n",
    "    #print(img_area)\n",
    "    #print(img.shape[:2]) -> W,H -> W*H\n",
    "    candidates = []\n",
    "    for r in regions:\n",
    "        if r['rect'] in candidates: continue\n",
    "        if r['size'] < (0.01*img_area): continue # 너무 작은 값 pass\n",
    "        if r['size'] > (1*img_area): continue #너무 큰 값 pass\n",
    "        x, y, w, h = r['rect']\n",
    "        #print(x,y,w,h)\n",
    "        candidates.append(list(r['rect']))\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a2c7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_iou(boxA, boxB, epsilon=1e-5):\n",
    "    #두 영역의 교집합 구하기\n",
    "    x1 = max(boxA[0], boxB[0])\n",
    "    y1 = max(boxA[1], boxB[1])\n",
    "    x2 = min(boxA[2], boxB[2])\n",
    "    y2 = min(boxA[3], boxB[3])\n",
    "    width = (x2 - x1)\n",
    "    height = (y2 - y1)\n",
    "    if (width<0) or (height <0):\n",
    "        return 0.0\n",
    "    area_overlap = width * height #교집합 넓이\n",
    "    area_a = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1]) #a의 넓이\n",
    "    area_b = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1]) #b의 넓이\n",
    "    area_combined = area_a + area_b - area_overlap #합집합 넓이\n",
    "    iou = area_overlap / (area_combined+epsilon) # 교집합/합집합 ->비율\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adf17cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(show)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b790023e",
   "metadata": {},
   "source": [
    "### 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4449080",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#예시\n",
    "(im, bbs, labels, fpath) = ds[4456]\n",
    "H, W,_= im.shape\n",
    "candidates = extract_candidates(im)\n",
    "candidates = np.array([(x,y,x+w,y+h) for x,y,w,h in candidates])  # ds에서 Region proposal된 모든 bbs 가져옴\n",
    "\n",
    "ious, rois, clss, deltas, best_ious = [], [], [], [], []\n",
    "temp_best_bbs = []\n",
    "ious = np.array([[extract_iou(candidate, _bb_) for candidate in candidates] for _bb_ in bbs]).T\n",
    "\n",
    "for jx, candidate in enumerate(candidates):\n",
    "    cx,cy,cX,cY = candidate\n",
    "    candidate_ious = ious[jx]  #Region proposal된 모든 bbs 반복\n",
    "    #print(candidate_ious)\n",
    "    best_iou_at = np.argmax(candidate_ious)  #best candidate iou is taken (index) ~ 항상 0나옴\n",
    "    best_iou = candidate_ious[best_iou_at]   #gets the best score here\n",
    "    #print(best_iou)\n",
    "    best_ious.append(best_iou)\n",
    "    best_bb = _x,_y,_X,_Y = bbs[best_iou_at] # gets the target label bounding box where there is the highest iou\n",
    "    #print(best_bb)\n",
    "    temp_best_bbs.append(best_bb)\n",
    "    if best_iou > 0.3: clss.append(labels[best_iou_at]) # 30퍼센트이상 포함되어야함\n",
    "    else : clss.append('background')\n",
    "    delta = np.array([_x-cx, _y-cy, _X-cX, _Y-cY]) / np.array([W,H,W,H])  #값 정규화\n",
    "    deltas.append(delta)\n",
    "    rois.append(candidate / np.array([W,H,W,H]))\n",
    "\n",
    "best_ious_at = np.argmax(best_ious)\n",
    "print(\"Best IoU:\", best_ious[best_ious_at])\n",
    "\n",
    "\n",
    "best_candidate = candidates[best_ious_at]\n",
    "best_bbs = temp_best_bbs[best_ious_at]\n",
    "\n",
    "# Example of df[15]\n",
    "candidates = extract_candidates(im)\n",
    "show(im, bbs = [best_bbs, best_candidate], confs= [0,0.5], texts = ['Bbox', 'Best candidate Bbox'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b7b82d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Region Proposal 결과\n",
    "candidates = extract_candidates(im)\n",
    "print(np.shape(candidates))\n",
    "print(candidates)\n",
    "show(im, bbs = candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323ba025",
   "metadata": {},
   "outputs": [],
   "source": [
    "FPATHS, GTBBS, CLSS, DELTAS, ROIS, IOUS = [], [], [], [], [], []\n",
    "#N = 1\n",
    "N = 500\n",
    "for ix, (im, bbs, labels, fpath) in enumerate(ds): \n",
    "    if(ix==N):\n",
    "        break\n",
    "    H, W, _ = im.shape #이미지의 높이,너비,채널\n",
    "    #print(H,W,_)\n",
    "    #사용함수 : extract_candidates(img)\n",
    "    candidates = extract_candidates(im)  \n",
    "    #print(candidates)\n",
    "    candidates = np.array([(x,y,x+w,y+h) for x,y,w,h in candidates]) #x,y ->bb의 좌상단 좌표\n",
    "    #print(candidates)\n",
    "    ious, rois, clss, deltas = [], [], [], []\n",
    "    #사용함수 : extract_iou(boxA, boxB, epsilon=1e-5)\n",
    "    ious = np.array([[extract_iou(candidate, _bb_) for candidate in candidates] for _bb_ in bbs]).T\n",
    "    #print(ious)\n",
    "    for jx, candidate in enumerate(candidates):\n",
    "        cx,cy,cX,cY = candidate\n",
    "        #print(cx,cy,cX,cY)\n",
    "        candidate_ious = ious[jx]\n",
    "        best_iou_at = np.argmax(candidate_ious)\n",
    "        best_iou = candidate_ious[best_iou_at]\n",
    "        best_bb = _x,_y,_X,_Y = bbs[best_iou_at]\n",
    "        if best_iou > 0.3: clss.append(labels[best_iou_at])\n",
    "        else : clss.append('background')\n",
    "        delta = np.array([_x-cx, _y-cy, _X-cX, _Y-cY]) / np.array([W,H,W,H]) #정규화\n",
    "        deltas.append(delta)\n",
    "        rois.append(candidate / np.array([W,H,W,H]))\n",
    "    FPATHS.append(fpath)\n",
    "    IOUS.append(ious)\n",
    "    ROIS.append(rois)\n",
    "    CLSS.append(clss)\n",
    "    DELTAS.append(deltas)\n",
    "    GTBBS.append(bbs)\n",
    "FPATHS = [f'{image_root}/{stem(f)}.jpg' for f in FPATHS] \n",
    "FPATHS, GTBBS, CLSS, DELTAS, ROIS = [item for item in [FPATHS, GTBBS, CLSS, DELTAS, ROIS]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed6b19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DELTAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa78bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = pd.DataFrame(flatten(CLSS), columns=['label'])\n",
    "label2target = {l:t for t,l in enumerate(targets['label'].unique())}\n",
    "target2label = {t:l for l,t in label2target.items()}\n",
    "background_class = label2target['background']\n",
    "\n",
    "\n",
    "print(\"The label to target values dictionary formed is:\" ,label2target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20ccf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                  std=[0.229, 0.224, 0.225])\n",
    "# normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "#                                  std=[0.5, 0.5, 0.5])\n",
    "def preprocess_image(img):\n",
    "    #RGB to BGR\n",
    "    img = torch.tensor(img).permute(2,0,1)\n",
    "    img = normalize(img)\n",
    "    return img.to(device).float()\n",
    "def decode(_y):\n",
    "    _, preds = _y.max(-1)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e334476",
   "metadata": {},
   "source": [
    "### 4. Fast R-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd40b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FRCNNDataset(Dataset):\n",
    "    def __init__(self, fpaths, rois, labels, deltas, gtbbs):\n",
    "        self.fpaths = fpaths\n",
    "        self.gtbbs = gtbbs\n",
    "        self.rois = rois\n",
    "        self.labels = labels\n",
    "        self.deltas = deltas\n",
    "    def __len__(self): return len(self.fpaths)\n",
    "    def __getitem__(self, ix):\n",
    "        fpath = str(self.fpaths[ix])\n",
    "        image = cv2.imread(fpath, 1)[...,::-1] # convert BGR to RGB\n",
    "        gtbbs = self.gtbbs[ix]\n",
    "        rois = self.rois[ix]\n",
    "        labels = self.labels[ix]\n",
    "        deltas = self.deltas[ix]\n",
    "        assert len(rois) == len(labels) == len(deltas), f'{len(rois)}, {len(labels)}, {len(deltas)}'\n",
    "        return image, rois, labels, deltas, gtbbs, fpath\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        input, rois, rixs, labels, deltas = [], [], [], [], []\n",
    "        for ix in range(len(batch)):\n",
    "            image, image_rois, image_labels, image_deltas, image_gt_bbs, image_fpath = batch[ix]\n",
    "            image = cv2.resize(image, (224,224))\n",
    "            input.append(preprocess_image(image/255.)[None])\n",
    "            rois.extend(image_rois)\n",
    "            rixs.extend([ix]*len(image_rois))\n",
    "            labels.extend([label2target[c] for c in image_labels])\n",
    "            deltas.extend(image_deltas)\n",
    "        input = torch.cat(input).to(device)\n",
    "        rois = torch.Tensor(rois).float().to(device)\n",
    "        rixs = torch.Tensor(rixs).float().to(device)\n",
    "        labels = torch.Tensor(labels).long().to(device)\n",
    "        deltas = torch.Tensor(deltas).float().to(device)\n",
    "        return input, rois, rixs, labels, deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de255c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data의 90% ->Train 나머지 Test\n",
    "n_train = 9*len(FPATHS)//10\n",
    "print(n_train)\n",
    "train_ds = FRCNNDataset(FPATHS[:n_train], ROIS[:n_train], CLSS[:n_train], DELTAS[:n_train], GTBBS[:n_train])\n",
    "test_ds = FRCNNDataset(FPATHS[n_train:], ROIS[n_train:], CLSS[n_train:], DELTAS[n_train:], GTBBS[n_train:])\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_loader = DataLoader(train_ds, batch_size=4, collate_fn=train_ds.collate_fn, drop_last=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=4, collate_fn=test_ds.collate_fn, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9e9b3c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for input, rois, rixs, labels, deltas in train_loader:\n",
    "    print(input.shape)\n",
    "    print(labels) # labels 값이 각 batch_size마다 다름\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdf952d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import RoIPool\n",
    "class FRCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        rawnet = torchvision.models.vgg16_bn(pretrained=True)#backbone\n",
    "        for param in rawnet.features.parameters():\n",
    "            param.requires_grad = True\n",
    "        self.seq = nn.Sequential(*list(rawnet.features.children())[:-1])\n",
    "        self.roipool = RoIPool(7, spatial_scale=14/224)\n",
    "        feature_dim = 512*7*7\n",
    "        self.cls_score = nn.Linear(feature_dim, len(label2target))\n",
    "        self.bbox = nn.Sequential(\n",
    "              nn.Linear(feature_dim, 512),\n",
    "              nn.ReLU(),\n",
    "              nn.Linear(512, 4),\n",
    "              nn.Tanh(),\n",
    "            )\n",
    "        self.cel = nn.CrossEntropyLoss()\n",
    "        self.sl1 = nn.SmoothL1Loss()\n",
    "    def forward(self, input, rois, ridx):\n",
    "        res = input\n",
    "        res = self.seq(res)\n",
    "        rois = torch.cat([ridx.unsqueeze(-1), rois*224], dim=-1)\n",
    "        res = self.roipool(res, rois)\n",
    "        feat = res.view(len(res), -1)\n",
    "        cls_score = self.cls_score(feat)\n",
    "        bbox = self.bbox(feat) # .view(-1, len(label2target), 4)\n",
    "        return cls_score, bbox\n",
    "    def calc_loss(self, probs, _deltas, labels, deltas):\n",
    "        detection_loss = self.cel(probs, labels)\n",
    "        ixs, = torch.where(labels != background_class)\n",
    "        _deltas = _deltas[ixs]\n",
    "        deltas = deltas[ixs]\n",
    "        self.lmb = 10.0\n",
    "        if len(ixs) > 0:\n",
    "            regression_loss = self.sl1(_deltas, deltas)\n",
    "            return detection_loss + self.lmb * regression_loss, detection_loss.detach(), regression_loss.detach()\n",
    "        else:\n",
    "            regression_loss = 0\n",
    "            return detection_loss + self.lmb * regression_loss, detection_loss.detach(), regression_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fa467c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "frcnn = FRCNN().to(device)\n",
    "print(frcnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71293012",
   "metadata": {},
   "source": [
    "### 5.Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b0554a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(inputs, model, optimizer, criterion):\n",
    "    input, rois, rixs, clss, deltas = inputs\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    _clss, _deltas = model(input, rois, rixs)\n",
    "    loss, loc_loss, regr_loss = criterion(_clss, _deltas, clss, deltas)\n",
    "    accs = clss == decode(_clss)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.detach(), loc_loss, regr_loss, accs.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67aa7836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_batch(inputs, model, criterion):\n",
    "    input, rois, rixs, clss, deltas = inputs\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        _clss,_deltas = model(input, rois, rixs)\n",
    "        loss, loc_loss, regr_loss = criterion(_clss, _deltas, clss, deltas)\n",
    "        _clss = decode(_clss)\n",
    "        accs = clss == _clss\n",
    "    return _clss, _deltas, loss.detach(), loc_loss, regr_loss, accs.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2875dce1",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20910fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frcnn = FRCNN().to(device)\n",
    "criterion = frcnn.calc_loss\n",
    "optimizer = optim.Adam(frcnn.parameters(), lr=0.0001)\n",
    "\n",
    "n_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc59f0c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "log = Report(n_epochs)\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    _n = len(train_loader)\n",
    "    for ix, inputs in enumerate(train_loader):\n",
    "        loss, loc_loss, regr_loss, accs = train_batch(inputs, frcnn, \n",
    "                                                      optimizer, criterion)\n",
    "        pos = (epoch + (ix+1)/_n)\n",
    "        log.record(pos, trn_loss=loss.item(), trn_loc_loss=loc_loss, \n",
    "                   trn_regr_loss=regr_loss, \n",
    "                   trn_acc=accs.mean(), end='\\r')\n",
    "        \n",
    "    _n = len(test_loader)\n",
    "    for ix,inputs in enumerate(test_loader):\n",
    "        _clss, _deltas, loss, \\\n",
    "        loc_loss, regr_loss, accs = validate_batch(inputs, \n",
    "                                                frcnn, criterion)\n",
    "        pos = (epoch + (ix+1)/_n)\n",
    "        log.record(pos, val_loss=loss.item(), val_loc_loss=loc_loss, \n",
    "                val_regr_loss=regr_loss, \n",
    "                val_acc=accs.mean(), end='\\r')\n",
    "        \n",
    "    log.report_avgs(epoch+1)\n",
    "\n",
    "# Plotting training and validation metrics\n",
    "log.plot_epochs('trn_loss,val_loss'.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a22183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "b = time.time()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9fcf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "b-a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8fda5f",
   "metadata": {},
   "source": [
    "### 6. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55336417",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image = cv2.imread(image_path, 1)[...,::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ace77e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.patches as mpatches\n",
    "from torchvision.ops import nms\n",
    "from PIL import Image\n",
    "def test_predictions(filename):\n",
    "    img = cv2.resize(cv2.imread(filename, 1)[...,::-1], (224,224))\n",
    "    #print(img.shape)\n",
    "    candidates = extract_candidates(img)\n",
    "    candidates = [(x,y,x+w,y+h) for x,y,w,h in candidates]\n",
    "    input = preprocess_image(img/255.)[None]\n",
    "    rois = [[x/224,y/224,X/224,Y/224] for x,y,X,Y in candidates]\n",
    "    rixs = np.array([0]*len(rois))\n",
    "    rois, rixs = [torch.Tensor(item).to(device) for item in [rois, rixs]]\n",
    "    with torch.no_grad():\n",
    "        frcnn.eval() ##eval() -> gradient 고정\n",
    "        probs, deltas = frcnn(input, rois, rixs)\n",
    "        probs = torch.nn.functional.softmax(probs, -1)\n",
    "        confs, clss = torch.max(probs, -1)\n",
    "    candidates = np.array(candidates)\n",
    "    confs, clss, probs, deltas = [tensor.detach().cpu().numpy() for tensor in [confs, clss, probs, deltas]]\n",
    "    \n",
    "    ixs = clss!=background_class\n",
    "    confs, clss, probs, deltas, candidates = [tensor[ixs] for tensor in [confs, clss, probs, deltas, candidates]]\n",
    "    bbs = candidates + deltas\n",
    "    ixs = nms(torch.tensor(bbs.astype(np.float32)), torch.tensor(confs), 0.05)\n",
    "    confs, clss, probs, deltas, candidates, bbs = [tensor[ixs] for tensor in [confs, clss, probs, deltas, candidates, bbs]]\n",
    "    if len(ixs) == 1:\n",
    "        confs, clss, probs, deltas, candidates, bbs = [tensor[None] for tensor in [confs, clss, probs, deltas, candidates, bbs]]\n",
    "    \n",
    "    bbs = bbs.astype(np.uint16)\n",
    "    _, ax = plt.subplots(1, 2, figsize=(20,10))\n",
    "    show(img, ax=ax[0])\n",
    "    ax[0].grid(False)\n",
    "    ax[0].set_title(filename.split('/')[-1])\n",
    "    if len(confs) == 0:\n",
    "        ax[1].imshow(img)\n",
    "        ax[1].set_title('No objects')\n",
    "        plt.show()\n",
    "        return\n",
    "    else:\n",
    "        show(img, bbs=bbs.tolist(), texts=[target2label[c] for c in clss.tolist()], ax=ax[1])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f07c03",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_predictions(test_ds[10][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4aa0916",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(500):\n",
    "    test_predictions(test_ds[i][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bc677d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_predictions(test_ds[49][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bc4950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(frcnn.state_dict(), f'./frcnn_bus_truck_vgg16.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f14a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = OpenImages(df=data)\n",
    "im, bbs, clss, _ = ds[460] #image(RGB), boxes(여러개), lablename, path\n",
    "#print(im)\n",
    "#print(type(bbs))\n",
    "#print(ds[2])\n",
    "show(im, bbs=bbs, texts=clss, sz=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec7a43f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
